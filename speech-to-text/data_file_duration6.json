{"video2v001_0_0:00:59.970000_0:00:00": "for some reason or maybe they're too lazy to migrate over if someone from Google is watching this let us know enter Borg it's comparable to a really popular open-source tools that Google created can you guess which one let me give you a hint it's schedules and manager has hundreds of thousands of jobs spanning thousands of machines were a job is the smallest Deployable unit of computing it consists of one or more ask for a task is a runnable instance of a binary like a server or a batch job so if you guests kubernetes nice job in 2015 where a job is kind of like a kubernetes pod and a boring task is kind of like a kubernetes container but instead of Google is a culture of creating the most esoteric names possible for everything but I will say that Google I never saw anything that resembled a Docker file so this might be a case where the tech really just", "video2v001_0_0:01:59.940000_0:00:59.970000": "Works orgasm more parallels with kubernetes which you can read more about in the white paper but to summarize Google doesn't use the complicated kubernetes to instead you even more complicated yeah now finally the moment you've been waiting for databases big table was created with Google ran into the limitations of relational databases second and be extremely scalable tables of columns and rows related and family can I have multiple versions of a value so it's kind of like a three-dimensional table where the third dimension is time another difference from sequels that it's sparsely populated because not every column has to be required by each row have the flexibility to add columns as needed before being flushed", "video2v001_0_0:02:59.910000_0:01:59.940000": "the SS tables where they will be immutable like I said it is easy to 2006 big table paper inspired the 2007 Dynamo paper which later led to dynamodb also inspired the development of Cassandra and I'm sure Amazon continue to give back to the open source Community after that this is the part where I was going to have a mongodb sponsorship I'll just plug my site instead I recently added a full stack developer course where we build out a YouTube skeleton and we focus on the part that most people avoid the backend specifically focusing on the upload feature where videos are asynchronously transcoded and then serve two users you can read more about it in a short design doc I wrote which is free I think this is the type of project that most people definitely don't have on their resume so it might help set you apart and now number one right after a few honorable mentions spanner is the crackhead database that uses GPS and atomic clocks to literally break cap theorem", "video2v001_0_0:03:15.310000_0:02:59.910000": "bigquery the crown jewel of Google cloud and there's Blaze Google's build tool that I never fully learned how to use and was open sourced as Basile finally a name that makes sense and now for the final 01 Google domains just kidding it's dead", "video2v001_0_0:04:06.880000_0:03:15.310000": "let's finish with the most secretive tool on this list goop's okay seriously who's coming up with these names there's not much public info on it but Cloud Pub sub is the public version and is pretty much equivalent at its core Pub sub is a message to you so it's comparable the tools like rabbitmq and Kafka without a message to your architecture might look like this but if we want to handle high-throughput added durability and decouple our services we can introduce pubsub to our architecture to summarize we talked about how Google stores data while they were talking about files or transactional data we talked about how Google moves data with grpc protocol buffers and message to use we talked about how Google processes data originally with mapreduce and later Flume and lastly Borg which is how Google orchestrates all of the above thanks for watching this was a fun one and hopefully I'll see you soon", "video1v000_1_0:00:59.970000_0:00:00": "you guys know how seriously I take solving leetcode problems but people always say that personal projects are more important than solving leetcode problems so I decided to code Elite code cologne it took me three weeks to code this up and honestly it wasn't that hard I'll show you how I did it including the UI the back end and the overall process of adding a new problem let's start with my favorite part the back end ultimately we want user to be able to execute some arbitrary piece of code in other words we're building a code sandbox but we can't just run any code within a browser that would be a security issue and for that reason of browser itself is a Sandbox environment so we need to send the code to our own server but then one of you assholes are going to try to mine Bitcoin on my VM so it's another security issue so we have to execute this code in an isolated environment with the minimum amount of privileges even if I wanted to building something like this would take a lot of time so I followed the number one rule of programming when you see a piece of code you like", "video1v000_1_0:01:59.940000_0:00:59.970000": "you you ain't get I found judge 0 which is an open source code execution engine they also have an API what you can paper use but I decided to just self-host this so I had to decide which VM would be the best for my use case now obviously running multiple codes emissions especially at the same time is going to be cpu-intensive so I chose a VM with eight cores a python or JavaScript submission would only take about one and a half seconds and only a fraction of that time was actually spent executing code whereas C plus plus code took about two seconds and Jabba took two and a half maybe three seconds even though executing Java code is faster than python or JavaScript I assume most of that time was taken from the jvm start up in like the compilation and all that and I decided to run some benchmarks just to get a ballpark idea of how many requests I would be able to handle I didn't want to waste too much time so my methodology isn't the smartest but basically I had a while loop running a certain amount of submissions at like the exact same time I ran them with", "video1v000_1_0:02:59.910000_0:01:59.940000": "Python and Java can see the average latency for each request which doesn't really increase until we get to overrate concurrent request submitted at the exact same time and that makes sense cuz rvm has eight cores so theoretically we can have eight submissions running at the same time even with 32 concurrent request the average latency was only 4 seconds now this doesn't tell the whole picture because some of the requests finished in like one and a half seconds where are some of them took up to 8 seconds I repeated this with Java and it was definitely a lot slower in the worst case with 32 concurrent submissions the average request took about 10 seconds the slowest ones took about 19 seconds and this is a very very rough way of estimating the craze for a second but I don't think it's far off if we were able to process 32 request in 20 seconds then I think it's fair to say that our system can handle at least 90 requests per minute and I know that most of my users use Python so I can probably do a lot more than that", "video1v000_1_0:03:59.880000_0:02:59.910000": "frankly this is going to be more than enough with the amount of traffic that I get now switching over to the UI this was definitely the most annoying part I just hate front end development in general especially because I write pretty much all the CSS on my site and the most interesting part was probably the code editor which once again I just yanked specifically I use the Monica code editor I don't know if that's how you pronounce it but it's the same code editor that vs code uses and I thought if I just need a plug this in it's going to be really easy to use everything's going to work fine and I don't have to worry about it compared to using something like code mirror but that was actually not the case as is typical with front-end development I literally had to set up a custom webpack config which is not something you normally have to do with angular projects and even then I still had issues that just drove me crazy until I finally found a GitHub post which mentioned that you're supposed to downgrade a specific dependency to get it to work this is exactly why I prefer leetcode over", "video1v000_1_0:04:59.850000_0:03:59.880000": "software development yeah some algorithms definitely are hard as hell but at least you don't have to deal with random BS like this I also thought that I would easily be able to customize the editor theme to get it to look exactly like BS code but even that is actually a lot more complicated than I thought like I'm not able to find an example on the internet of someone doing this and I don't know if that's because I'm just dumb or because this is surprisingly more painful than you would expect somebody knows how to do it please let me know I'll even pay you the rest of the front and features weren't really hard it was just a lot to coat up to be honest like things like changing the font size going into full-screen mode especially like resizing everything that was a huge pain maybe I can explain how I did that in a future video or maybe a course or something like that now for the process of actually adding a new problem so far for the problems I've added the user is asked to design a day to structure which is encapsulated in a class and the process of testing this class involves taking some input.", "video2v000_2_0:00:59.970000_0:00:00": "I just quit my job at Google so now I'm going to expose all of their ultra-secret Technologies just kidding all of these can be found in this GitHub repo before Google was known for suckling on your sweet sweet data they were known for revolutionising distributed computing again and again and again and they did it by using 6 revolutionary tools that most people never heard of but I'm going to show them to you today let's start with a classic the Google file system its 2003 most people are still learning how to open their file explorer but Google has a different problem search engines need a web crawler could navigate through every web page available and save it so that it can be converted to a weighted graph and used within pagerank now that's quite a lot of data to store today that would be on the order of petabytes of data on top of that these files need to be concurrently read and written to buy mini machines from many developers at Google requiring High throughput consistent data and replicated", "video2v000_2_0:01:59.940000_0:00:59.970000": "the high-level implementation is that files are split into 64 MB chunks before they are stored each chunk is assigned a universally unique ID and a given chunk is not only stored on one server but it's stored on at least three servers there's also a single Master server which sort of acts as the table of contents it tells you the directory structure map every file to a list of its corresponding chunks and of course tells you the chunk locations as well it kind of reminds me of the super Block in a Linux file system but in this case are filed chunks are distributed which makes things a lot more complicated now Google didn't just keep their secrets to themselves they published a very revolutionary paper which inspired engineers at Yahoo to develop the Hadoop distributed file system which was later open-sourced the original GFS was also succeeded by Google's Colossus file system okay but storing data is easy how do we actually process the raw webpages well convenience", "video2v000_2_0:02:59.910000_0:01:59.940000": "a year later Google release the mapreduce white paper the problem is that you have to process a lot of data like from Google file system and you could use a single machine to do it but then you might never go home instead you can use the most powerful big data processing framework at the time that your company just happened to invent and don't worry it's actually pretty simple as the name implies there's two main steps mapping the data and reducing the data at least from your perspective as a programmer but there's a hidden middle step called the shuffle or sort step let's say our input is a bunch of raw text files for the map step we would split our data up into individual chunks and each server would receive a portion of them the output from each server would be a list of intermediate key-value pairs now before the reduce step we will Shuffle or sort the data by making sure that every pair with the same key value ends up at the same server for the reduce step this is important because", "video2v000_2_0:03:59.880000_0:02:59.910000": "when we reduce or aggregate our data we do so by the key for the input here is the intermediate Ki value Paris and the output is the final key-value pairs congratulations you just wrote a distributed program to count Words exciting but the reason mapreduce was so revolutionary is that from a programmer's perspective you are only responsible for implementing two functions and these are analogous to the map and reduce functions from functional programming you didn't have to be a distributed systems expert to use this text and shortly after Hadoop mapreduce and open-source variation was released nowadays no one really uses mapreduce anymore Google uses Flume Apache beam as the open source equivalent and cloud data flow is the Google manage job Runner meanwhile the rest of the world uses the Patxi's spark and flank for the same purposes okay enough about infrastructure show me some code will you probably heard of restful apis but if you really want to bust your balls watch me build the same API", "video2v000_2_0:04:59.850000_0:03:59.880000": "PC were using pro version 3 and this is our hello world package we start by defining are schemas for our PCS requests and responses then say hello will basically receive a name and return a greeting message like importing the protocol buffers that we register our helloworld RPC Handler and create a grp server which listens for request now that's definitely a lot more work than supported from browser now if you were paying attention that you might have noticed that we actually have scheme as and type safety that's one of the purposes of protocol buffers it might be less obvious though that grpc is more efficient because data is binary serialized rather than being human-readable like Jason okay but what is this basically it's the Google internal version of grpc", "video1v001_3_0:00:59.970000_0:00:00": "get into the class and then comparing the output with the expected output there's a lot of ways you can probably do this you can even use like a unit testing framework probably but I tried to keep things as simple as possible so I actually wrote a driver code file for every single problem and I took the user's code and actually concatenated it to the driver code file so that everything would be in a single file and then I could just take that one file and then send it over to judge zero and then execute the driver could file specifically parses the test input based on that it chooses which method to run of the class in which parameters to pass it and then it takes the output and puts it into a consistent format because I didn't want to define a specific output for every single language for example some languages print are raised differently summer language don't even let you print a raise so that's kind of the responsibility of the driver code now when it comes to sending this to judge zero you can send it in a single submission or you can send a batch submission you would think we would want to do", "video1v001_3_0:01:59.940000_0:00:59.970000": "batch submission meaning one submission for every single test case programming that is actually a bit more straightforward but the problem that I ran into is that that massively increases the latency and it makes sense why that is because as we talked about earlier when we're executing Java code most of the latency comes from compiling the code and probably starting up the jvm and all that so we have to do that like a dozen times one for each test case it's going to slow our code down a lot but if we only have to do it a single time meaning compiling the code and starting it up and then we can just run every single test case and compare each of those with the expected output that's going to be a lot faster now the one issue with this is at least the way I coded it up each test case output is printed onto a single line so if you as a user have some like log statements in your code as of right now you're not going to be able to see them while you might see them but it's going to mess up the rest of the test cases but I already know", "video1v001_3_0:02:59.910000_0:01:59.940000": "exactly how to fix that it's just something I'm going to have to do and that actually brings me to another limitation we're right now you as a user can submit your own custom test case the reason for this is for all of the test cases I created I already have the expected output but if you give me a custom test case I don't necessarily know what the expected output is going to be so what I have to do is take my solution code or some solution code run it against your custom test case and run your code against that custom test case and then compare the difference if the output is the same then your code passes be custom test case this is again not like complicated to coat up it's just another thing that you have to actually do it even though I said that this whole thing was easier to do than I expected it still is a lot of work as a lot of like features as a lot of edge cases and things to do but now that I've pretty much laid the entire Foundation of this the possibilities are Limitless like I have a dozen ideas of things that I want to start working on now if you're wondering how", "video1v001_3_0:03:59.880000_0:02:59.910000": "I did all of this so quickly well I had a lot of support from my manager the p.m. the dev team that I work with his really good the CEO likes me the joke is that I'm all of those people cuz I did all this myself but I am getting to the point where I probably do need help the biggest shortcut I probably took and don't tell your Tech lead I told you this but I actually didn't write unit tests at least in my case I think it's a waste of time because I already know like the entire code base I think a lot of times unit test sort of serve as documentation for new Debs and in my case if I update like a certain piece of code I already know all of the other pieces of code that might be affected so I can quickly just manually test that everything still works and even when there is a bug which is pretty rare in my case in those cases it's not something a unit test would usually catches usually like a race condition or something like that now if you're thinking wait a minute I thought you just sat in your basement and solve leetcode problems all day but yes that's true I try to upload leetcode problems regularly on my second", "video1v001_3_0:04:30.750000_0:03:59.880000": "Channel but maybe just maybe there is some correlation with being good at solving leetcode problems especially leetcode hard and being a good developer or at least being able to become a good developer like I see people on Twitter complaining about next js13 oh no the app router it's so complicated guys like I've been doing angular and I'm not saying that like it's a good thing but I'm just saying that maybe being a real Dev isn't as hard as people make it out to be"}